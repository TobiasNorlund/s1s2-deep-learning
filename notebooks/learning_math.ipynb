{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, models\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '+',\n",
       " '=',\n",
       " 'Q',\n",
       " 'E',\n",
       " 'F',\n",
       " '!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABULARY = [str(d) for d in range(10)] + ['+', '=', 'Q', 'E', 'F', '!']\n",
    "VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'97+53Q0+7+3E10Q1+9+5E15F150!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data(n_samples, max_n_digits, seed, min_n_digits=1):\n",
    "    random.seed(seed)\n",
    "    already_generated = set()\n",
    "    \n",
    "    while True:\n",
    "        if len(already_generated) >= n_samples:\n",
    "            break\n",
    "        \n",
    "        n_digits = random.randint(min_n_digits, max_n_digits)\n",
    "        \n",
    "        a = random.randint(0, 10 ** n_digits - 1)\n",
    "        b = random.randint(0, 10 ** n_digits - 1)\n",
    "        \n",
    "        if (a, b) in already_generated:\n",
    "            continue\n",
    "        \n",
    "        already_generated.add((a, b))\n",
    "        \n",
    "        problem = f'{a:0{n_digits}d}+{b:0{n_digits}d}'\n",
    "\n",
    "        carry = 0\n",
    "        \n",
    "        for significance in range(n_digits):\n",
    "            a_digit = (a // 10 ** significance) % 10\n",
    "            b_digit = (b // 10 ** significance) % 10\n",
    "            result = a_digit + b_digit + carry\n",
    "            result_digit = result % 10\n",
    "            result_carry = result // 10\n",
    "            problem += f'Q{carry}+{a_digit}+{b_digit}E{result_carry}{result_digit}'\n",
    "            carry = result_carry\n",
    "        \n",
    "        problem += f'F{a+b}!'\n",
    "        \n",
    "        yield problem\n",
    "\n",
    "next(data(100, 3, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    s = list(s)\n",
    "    return np.array([VOCABULARY.index(c) for c in s], dtype=np.int32)\n",
    "\n",
    "def detokenize(arr):\n",
    "    return ''.join([VOCABULARY[x] for x in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'08'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def system_1(s):\n",
    "    return f'{eval(s):02d}'\n",
    "\n",
    "system_1('7+1+0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(dataset):\n",
    "    for example in dataset:\n",
    "        idxs = [(m.start(0), m.end(0))\n",
    "                for m in re.finditer('Q.\\+.\\+.E|F.*!', example[1:])]\n",
    "        \n",
    "        tokens = tokenize(example)\n",
    "        inputs = tokens[:-1]\n",
    "        targets = tokens[1:]\n",
    "        \n",
    "        mask = np.array([0] * len(targets))\n",
    "        for s, e in idxs:\n",
    "            mask[s:e] = 1\n",
    "        \n",
    "        yield inputs, targets, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, a, b):\n",
    "    n_digits = max(len(str(a)), len(str(b)))\n",
    "    ex = f\"{a:0{n_digits}d}+{b:0{n_digits}d}\"\n",
    "    tokenized = tokenize(ex).astype(np.int32)\n",
    "    \n",
    "    while True:\n",
    "        while not tokenized[-1] in (VOCABULARY.index('E'), VOCABULARY.index('!')):\n",
    "            pred = model(tokenized[np.newaxis, ...])\n",
    "            pred = pred.numpy()\n",
    "            next_token = np.argmax(pred, axis=-1)[0, -1].astype(np.int32)\n",
    "            tokenized = np.append(tokenized, next_token)\n",
    "\n",
    "        if tokenized[-1] == VOCABULARY.index('E'):\n",
    "            # Call system 1\n",
    "            query = detokenize(tokenized)[-6:-1]\n",
    "            ans = system_1(query)\n",
    "            tokenized_ans = tokenize(ans)\n",
    "            tokenized = np.append(tokenized, tokenized_ans)\n",
    "\n",
    "        elif tokenized[-1] == VOCABULARY.index('!'):\n",
    "            break\n",
    "\n",
    "    return detokenize(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset):\n",
    "    n_correct = 0\n",
    "    for example in test_dataset:\n",
    "        i = example.index('Q')\n",
    "        ab = example[:i]\n",
    "        i = ab.index('+')\n",
    "        a = int(ab[:i])\n",
    "        b = int(ab[i:])\n",
    "\n",
    "        try:\n",
    "            ans = decode(model, a, b)\n",
    "            i = ans.index('F')\n",
    "            ans = ans[i+1:-1]\n",
    "            ans = int(ans)\n",
    "\n",
    "            if ans == a + b:\n",
    "                n_correct += 1\n",
    "        except ValueError as err:\n",
    "            print(\"ValueError in eval:\", err)\n",
    "        except SyntaxError as err:\n",
    "            print(\"SyntaxError in eval:\", err)\n",
    "\n",
    "    return n_correct / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size):\n",
    "    inputs = layers.Input((None,), dtype=tf.int32)\n",
    "    x = inputs\n",
    "    x = layers.Embedding(vocab_size, 64)(x)\n",
    "    x = layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "    model = models.Model(inputs, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_model(vocab_size):\n",
    "    inputs = layers.Input((None,), dtype=tf.int32)\n",
    "    x = inputs\n",
    "    x = layers.Embedding(vocab_size, 64)(x)\n",
    "    x = layers.LSTM(128, return_sequences=True)(x)\n",
    "    \n",
    "    # Attention mechanism.\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    keys = layers.Dense(128)(x)\n",
    "    look_ahead_mask = (1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)) * -1e9\n",
    "    look_ahead_mask = look_ahead_mask[tf.newaxis, ...]\n",
    "    attention_scores = x @ tf.transpose(keys, [0, 2, 1])\n",
    "    attention_scores = attention_scores + look_ahead_mask\n",
    "    attention_weights = tf.nn.softmax(attention_scores, name=\"attention_weights\")\n",
    "    x_att = attention_weights @ x\n",
    "    x = layers.Concatenate()([x, x_att])\n",
    "    \n",
    "    x = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "    model = models.Model(inputs, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_copy_model(vocab_size):\n",
    "    inputs = layers.Input((None,), dtype=tf.int32)\n",
    "    x = inputs\n",
    "    x = layers.Embedding(vocab_size, 64)(x)\n",
    "    x = layers.LSTM(128, return_sequences=True)(x)\n",
    "    \n",
    "    batch_size = tf.shape(x)[0]\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    keys = layers.Dense(128)(x)\n",
    "    look_ahead_mask = (1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)) * -1e9\n",
    "    look_ahead_mask = look_ahead_mask[tf.newaxis, ...]\n",
    "    attention_scores = x @ tf.transpose(keys, [0, 2, 1])\n",
    "    attention_scores = attention_scores + look_ahead_mask\n",
    "    attention_weights = tf.nn.softmax(attention_scores, name=\"attention_weights\")\n",
    "    x_att = attention_weights @ x\n",
    "    x = layers.Concatenate()([x, x_att])\n",
    "    \n",
    "    # Generative distribution.\n",
    "    gen_dist = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # Copy mechanism based distribution.\n",
    "    indices = tf.range(batch_size, dtype=tf.int32)[:, tf.newaxis]  # (bs, 1)\n",
    "    indices = tf.tile(indices, [1, seq_len])  # (bs, seq_len)\n",
    "    indices = tf.stack((indices, inputs), axis=2)\n",
    "    #indices = tf.transpose(indices, perm=[1, 2, 0])  # (bs, seq_len, 2)\n",
    "    updates = tf.transpose(attention_weights, perm=[0, 2, 1])\n",
    "    output_shape = [batch_size, vocab_size, seq_len]\n",
    "    copy_dist = tf.scatter_nd(indices, updates, output_shape)\n",
    "    copy_dist = tf.transpose(copy_dist, [0, 2, 1])\n",
    "    \n",
    "    # Combine distributions.\n",
    "    p_gen_pre_logit = layers.Concatenate()([x_att, keys])\n",
    "    p_gen = layers.Dense(1, activation=\"sigmoid\")(p_gen_pre_logit)\n",
    "    dist = p_gen * gen_dist + (1 - p_gen) * copy_dist \n",
    "    \n",
    "    model = models.Model(inputs, dist)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type):\n",
    "    vocab_size = len(VOCABULARY)\n",
    "    if model_type == \"baseline\":\n",
    "        model = build_model(vocab_size)\n",
    "    elif model_type == \"attention\":\n",
    "        model = build_attention_model(vocab_size)\n",
    "    elif model_type == \"copy\":\n",
    "        model = build_copy_model(vocab_size)\n",
    "    else:\n",
    "        raise Exception(\"Invalid model.\")\n",
    "    model.build(input_shape=(None, None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_experiment(train_dataset, test_dataset, model):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    batch_size = 64\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(train_dataset),\n",
    "        output_types=(tf.int32, tf.int32, tf.int32),\n",
    "        output_shapes=(\n",
    "            tf.TensorShape((None,)), \n",
    "            tf.TensorShape((None,)), \n",
    "            tf.TensorShape((None,))\n",
    "        ))\n",
    "    train_ds = train_ds.shuffle(10000)\n",
    "    train_ds = train_ds.padded_batch(batch_size, padded_shapes=([None], [None], [None]))\n",
    "\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    optimizer = optimizers.Adam(1e-3)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(inputs, targets, mask):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(inputs)\n",
    "            loss = tf.losses.sparse_categorical_crossentropy(targets, outputs)\n",
    "            loss = tf.boolean_mask(loss, tf.cast(mask, tf.bool))\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "            masked_outputs = tf.boolean_mask(outputs, tf.cast(mask, tf.bool))\n",
    "            accuracy(targets, outputs, sample_weight=mask)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    accuracy_history = []\n",
    "    stop_criteria_last_n = 5\n",
    "    \n",
    "    for i, (inputs, targets, mask) in train_ds.repeat(None).take(4000).enumerate():\n",
    "        accuracy.reset_states()\n",
    "        loss = train_step(inputs, targets, mask)\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(\n",
    "                'Step', i.numpy(),\n",
    "                'Loss', loss.numpy(),\n",
    "                'Acc', accuracy.result().numpy())\n",
    "            accuracy_history.append(accuracy.result().numpy())\n",
    "            if len(accuracy_history) > stop_criteria_last_n:\n",
    "                accuracy_history = accuracy_history[1:]\n",
    "            if sum(accuracy_history) >= 1.0 * stop_criteria_last_n - 0.001:\n",
    "                break\n",
    "    \n",
    "    accuracy = evaluate(model, test_dataset)\n",
    "    \n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 Loss 1.3457814 Acc 0.64203954\n",
      "Step 200 Loss 0.70420176 Acc 0.75947994\n",
      "Step 300 Loss 0.58641523 Acc 0.8057022\n",
      "Step 400 Loss 0.43101612 Acc 0.86687964\n",
      "Step 500 Loss 0.30489555 Acc 0.9095634\n",
      "Step 600 Loss 0.16828047 Acc 0.96447796\n",
      "Step 700 Loss 0.082351364 Acc 0.9918284\n",
      "Step 800 Loss 0.043892827 Acc 1.0\n",
      "Step 900 Loss 0.025749868 Acc 1.0\n",
      "Step 1000 Loss 0.015854716 Acc 1.0\n",
      "Step 1100 Loss 0.011685202 Acc 1.0\n",
      "Step 1200 Loss 0.008689083 Acc 1.0\n",
      "ValueError in eval: invalid literal for int() with base 10: '1F99'\n",
      "Step 100 Loss 1.6255708 Acc 0.5972222\n",
      "Step 200 Loss 0.8392676 Acc 0.7048138\n",
      "Step 300 Loss 0.74444896 Acc 0.7283721\n",
      "Step 400 Loss 0.6709831 Acc 0.7647059\n",
      "Step 500 Loss 0.604929 Acc 0.78977275\n",
      "Step 600 Loss 0.543627 Acc 0.8221601\n",
      "Step 700 Loss 0.45679596 Acc 0.84768814\n",
      "Step 800 Loss 0.39452314 Acc 0.8764548\n",
      "Step 900 Loss 0.2841471 Acc 0.9134701\n",
      "Step 1000 Loss 0.1996432 Acc 0.9505494\n",
      "Step 1100 Loss 0.14698721 Acc 0.97077626\n",
      "Step 1200 Loss 0.09047778 Acc 0.98541474\n",
      "Step 1300 Loss 0.06620153 Acc 0.99816006\n",
      "Step 1400 Loss 0.040348504 Acc 0.99816006\n",
      "Step 1500 Loss 0.030836925 Acc 1.0\n",
      "Step 1600 Loss 0.022562651 Acc 1.0\n",
      "Step 1700 Loss 0.016367095 Acc 1.0\n",
      "Step 1800 Loss 0.013697576 Acc 1.0\n",
      "Step 1900 Loss 0.010717852 Acc 1.0\n",
      "ValueError in eval: invalid literal for int() with base 10: '11F'\n",
      "Step 100 Loss 1.6616179 Acc 0.584922\n",
      "Step 200 Loss 0.85898703 Acc 0.69613737\n",
      "Step 300 Loss 0.77266526 Acc 0.7239627\n",
      "Step 400 Loss 0.72177845 Acc 0.7460457\n",
      "Step 500 Loss 0.65901464 Acc 0.75652176\n",
      "Step 600 Loss 0.5964933 Acc 0.7895652\n",
      "Step 700 Loss 0.55826837 Acc 0.795671\n",
      "Step 800 Loss 0.4730455 Acc 0.833913\n",
      "Step 900 Loss 0.38387686 Acc 0.8791594\n",
      "Step 1000 Loss 0.29768595 Acc 0.91585475\n",
      "Step 1100 Loss 0.23270303 Acc 0.93926054\n",
      "Step 1200 Loss 0.18054163 Acc 0.96557814\n",
      "Step 1300 Loss 0.12447052 Acc 0.9785139\n",
      "Step 1400 Loss 0.09317327 Acc 0.98442906\n",
      "Step 1500 Loss 0.06692736 Acc 0.99036777\n",
      "Step 1600 Loss 0.04292915 Acc 0.9982699\n",
      "Step 1700 Loss 0.033376556 Acc 1.0\n",
      "Step 1800 Loss 0.024277141 Acc 0.9991251\n",
      "Step 1900 Loss 0.020824863 Acc 1.0\n",
      "Step 2000 Loss 0.016484713 Acc 1.0\n",
      "Step 2100 Loss 0.012737071 Acc 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-14579bbafec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"baseline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-51f654b44fbc>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(train_dataset, test_dataset, model)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e5aca6c24acc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0910984d7983>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, a, b)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVOCABULARY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCABULARY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnext_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001b[0;32m-> 1147\u001b[0;31m               **normal_lstm_kwargs)\n\u001b[0m\u001b[1;32m   1148\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         (last_output, outputs, new_h, new_c,\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, activation, recurrent_activation, mask, time_major, go_backwards, sequence_lengths)\u001b[0m\n\u001b[1;32m   1279\u001b[0m       \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgo_backwards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m       if sequence_lengths is not None else timesteps)\n\u001b[0m\u001b[1;32m   1282\u001b[0m   return (last_output, outputs, new_states[0], new_states[1],\n\u001b[1;32m   1283\u001b[0m           _runtime(_RUNTIME_CPU))\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4172\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m           **while_loop_kwargs)\n\u001b[0m\u001b[1;32m   4175\u001b[0m       \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   4156\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m         output, new_states = step_function(current_input,\n\u001b[0;32m-> 4158\u001b[0;31m                                            tuple(states) + tuple(constants))\n\u001b[0m\u001b[1;32m   4159\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4160\u001b[0m         \u001b[0mflat_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_tm1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    469\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m    470\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AddV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         tld.op_callbacks, x, y)\n\u001b[0m\u001b[1;32m    472\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = data(\n",
    "    n_samples=5000,\n",
    "    max_n_digits=2,\n",
    "    seed=0)\n",
    "dataset = list(dataset)\n",
    "\n",
    "test_dataset = dataset[-300:]\n",
    "\n",
    "n_train_sizes = [100, 500, 1000, 2000]\n",
    "accuracies = []\n",
    "\n",
    "for n_train in n_train_sizes:\n",
    "    train_dataset = dataset[:n_train]\n",
    "    model = get_model(model_type=\"baseline\")\n",
    "    accuracy, _ = run_experiment(train_dataset, test_dataset, model)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "plt.plot(n_train_sizes, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline + self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = data(\n",
    "    n_samples=5000,\n",
    "    max_n_digits=2,\n",
    "    seed=0)\n",
    "dataset = list(dataset)\n",
    "\n",
    "test_dataset = dataset[-300:]\n",
    "\n",
    "n_train_sizes = [100, 500, 1000, 2000]\n",
    "accuracies = []\n",
    "for n_train in n_train_sizes:\n",
    "    train_dataset = dataset[:n_train]\n",
    "    model = get_model(model_type=\"attention\")\n",
    "    accuracy, _ = run_experiment(train_dataset, test_dataset, model)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "plt.plot(n_train_sizes, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline + self attention + copy mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = data(\n",
    "    n_samples=5000,\n",
    "    max_n_digits=2,\n",
    "    seed=0)\n",
    "dataset = list(dataset)\n",
    "\n",
    "test_dataset = dataset[-300:]\n",
    "\n",
    "n_train_sizes = [20, 50, 75, 100]\n",
    "accuracies = []\n",
    "for n_train in n_train_sizes:\n",
    "    train_dataset = dataset[:n_train]\n",
    "    model = get_model(model_type=\"copy\")\n",
    "    accuracy, _ = run_experiment(train_dataset, test_dataset, model)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "plt.plot(n_train_sizes, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing num digits in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = data(\n",
    "    n_samples=20000,\n",
    "    max_n_digits=10,\n",
    "    seed=0)\n",
    "train_dataset = list(train_dataset)\n",
    "\n",
    "test_dataset = data(\n",
    "    n_samples=100,\n",
    "    max_n_digits=11,\n",
    "    min_n_digits=11,\n",
    "    seed=0)\n",
    "test_dataset = list(test_dataset)\n",
    "\n",
    "accuracy, model = run_experiment(train_dataset, test_dataset, model_type=\"copy\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decode(model, 11111, 22222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ex = \"123+567Q0+3+7E1\"\n",
    "tokenized = tokenize(ex)\n",
    "\n",
    "model2 = models.Model(\n",
    "    inputs=model.inputs,\n",
    "    outputs=[model.outputs, model.get_layer('tf_op_layer_add_1').output])\n",
    "\n",
    "pred, att_weights = model2(tokenized[np.newaxis, ...])\n",
    "print(\"Softmax output:\")\n",
    "print(pred[0][0, -1, :])\n",
    "print('Argmax token:', VOCABULARY[np.argmax(pred[0][0, -1, :])])\n",
    "\n",
    "print(\"Attention weights:\")\n",
    "plt.plot(att_weights[0, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.89, 0.9966666666666667]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing num digits in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 64)     1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, None, 128)    98816       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(3,)]               0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_packed (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ones (TensorFlowOpL [(None, None)]       0           tf_op_layer_packed[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatrixBandPart (Ten [(None, None)]       0           tf_op_layer_ones[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 128)    16512       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub (TensorFlowOpLa [(None, None)]       0           tf_op_layer_MatrixBandPart[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(None, 128, None)]  0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_1 (TensorFlowOp [(None, None)]       0           tf_op_layer_sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(3,)]               0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_matmul (TensorFlowO [(None, None, None)] 0           lstm[0][0]                       \n",
      "                                                                 tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(1, None, None)]    0           tf_op_layer_mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add (TensorFlowOpLa [(None, None, None)] 0           tf_op_layer_matmul[0][0]         \n",
      "                                                                 tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_range (TensorFlowOp [(None,)]            0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_attention_weights ( [(None, None, None)] 0           tf_op_layer_add[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(None, 1)]          0           tf_op_layer_range[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile/multiples (Ten [(2,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_matmul_1 (TensorFlo [(None, None, 128)]  0           tf_op_layer_attention_weights[0][\n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile (TensorFlowOpL [(None, None)]       0           tf_op_layer_strided_slice_3[0][0]\n",
      "                                                                 tf_op_layer_Tile/multiples[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 256)    0           tf_op_layer_matmul_1[0][0]       \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, None, 2)]    0           tf_op_layer_Tile[0][0]           \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_1 (Tensor [(None, None, None)] 0           tf_op_layer_attention_weights[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ScatterNd/shape (Te [(3,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 1)      257         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 256)    0           lstm[0][0]                       \n",
      "                                                                 tf_op_layer_matmul_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ScatterNd (TensorFl [(None, 16, None)]   0           tf_op_layer_stack[0][0]          \n",
      "                                                                 tf_op_layer_transpose_1[0][0]    \n",
      "                                                                 tf_op_layer_ScatterNd/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 16)     4112        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_1 (TensorFlowOp [(None, None, 1)]    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_2 (Tensor [(None, None, 16)]   0           tf_op_layer_ScatterNd[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_2 (TensorFlowOp [(None, None, 16)]   0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_3 (TensorFlowOp [(None, None, 16)]   0           tf_op_layer_sub_1[0][0]          \n",
      "                                                                 tf_op_layer_transpose_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_1 (TensorFlowOp [(None, None, 16)]   0           tf_op_layer_mul_2[0][0]          \n",
      "                                                                 tf_op_layer_mul_3[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 120,721\n",
      "Trainable params: 120,721\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data(\n",
    "    n_samples=20000,\n",
    "    max_n_digits=10,\n",
    "    seed=0)\n",
    "train_dataset = list(train_dataset)\n",
    "\n",
    "test_dataset = data(\n",
    "    n_samples=100,\n",
    "    max_n_digits=11,\n",
    "    min_n_digits=11,\n",
    "    seed=0)\n",
    "test_dataset = list(test_dataset)\n",
    "\n",
    "accuracy, model = run_experiment(train_dataset, test_dataset, model_type=\"copy\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(model, 11111, 22222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output:\n",
      "tf.Tensor(\n",
      "[5.4974299e-07 4.5491379e-06 2.3568574e-09 5.8792615e-10 3.0597482e-09\n",
      " 2.8557656e-09 1.7530151e-09 2.7995368e-09 2.7640148e-09 1.1759320e-08\n",
      " 5.5896015e-07 2.2194342e-11 9.8302680e-01 1.6953945e-02 1.3359379e-05\n",
      " 1.6310629e-07], shape=(16,), dtype=float32)\n",
      "Argmax token: Q\n",
      "Attention weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff289dafd30>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex = \"123+567Q0+3+7E1\"\n",
    "tokenized = tokenize(ex)\n",
    "\n",
    "model2 = models.Model(\n",
    "    inputs=model.inputs,\n",
    "    outputs=[model.outputs, model.get_layer('tf_op_layer_add_1').output])\n",
    "\n",
    "pred, att_weights = model2(tokenized[np.newaxis, ...])\n",
    "print(\"Softmax output:\")\n",
    "print(pred[0][0, -1, :])\n",
    "print('Argmax token:', VOCABULARY[np.argmax(pred[0][0, -1, :])])\n",
    "\n",
    "print(\"Attention weights:\")\n",
    "plt.plot(att_weights[0, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 Loss 1.0844855 Acc 0.7380952\n",
      "Step 200 Loss 0.0671819 Acc 0.99745333\n",
      "Step 300 Loss 0.020122066 Acc 0.99831367\n",
      "Step 400 Loss 0.046948962 Acc 0.9922747\n",
      "Step 500 Loss 0.008210928 Acc 0.99914676\n",
      "Step 600 Loss 0.0049689612 Acc 1.0\n",
      "Step 700 Loss 0.0041937996 Acc 1.0\n",
      "Step 800 Loss 0.0021872132 Acc 1.0\n",
      "Step 900 Loss 0.018684015 Acc 0.99743587\n",
      "Step 1000 Loss 0.0014263749 Acc 1.0\n",
      "Step 1100 Loss 0.0017907986 Acc 1.0\n",
      "Step 1200 Loss 0.00137163 Acc 1.0\n",
      "Step 1300 Loss 0.0043881163 Acc 0.99914384\n",
      "Step 1400 Loss 0.010941404 Acc 0.9965928\n",
      "Step 1500 Loss 0.0013565168 Acc 1.0\n",
      "Step 1600 Loss 0.0005495819 Acc 1.0\n",
      "Step 1700 Loss 0.0004790116 Acc 1.0\n",
      "Step 1800 Loss 0.00045244486 Acc 1.0\n",
      "Step 1900 Loss 0.00036784488 Acc 1.0\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "Accuracy with max 2 digits, evaluating on 3 digits: 0.01\n",
      "Step 100 Loss 0.02150888 Acc 0.997905\n",
      "Step 200 Loss 0.00885637 Acc 1.0\n",
      "Step 300 Loss 0.003935226 Acc 1.0\n",
      "Step 400 Loss 0.002551656 Acc 1.0\n",
      "Step 500 Loss 0.0030709805 Acc 0.99929875\n",
      "Step 600 Loss 0.0017359885 Acc 1.0\n",
      "ValueError in eval: invalid literal for int() with base 10: '806F806'\n",
      "ValueError in eval: invalid literal for int() with base 10: '776F776'\n",
      "ValueError in eval: invalid literal for int() with base 10: '891F891'\n",
      "ValueError in eval: invalid literal for int() with base 10: '552F552'\n",
      "ValueError in eval: invalid literal for int() with base 10: '611F618'\n",
      "ValueError in eval: invalid literal for int() with base 10: '921F921'\n",
      "ValueError in eval: invalid literal for int() with base 10: '967F967'\n",
      "ValueError in eval: invalid literal for int() with base 10: '96F965'\n",
      "ValueError in eval: invalid literal for int() with base 10: '611F614'\n",
      "ValueError in eval: invalid literal for int() with base 10: '809F809'\n",
      "ValueError in eval: invalid literal for int() with base 10: '1194F194'\n",
      "ValueError in eval: invalid literal for int() with base 10: '876F876'\n",
      "ValueError in eval: invalid literal for int() with base 10: '952F952'\n",
      "ValueError in eval: invalid literal for int() with base 10: '98F986'\n",
      "Accuracy with max 3 digits, evaluating on 4 digits: 0.02\n",
      "Step 100 Loss 0.011541928 Acc 1.0\n",
      "Step 200 Loss 0.0074725505 Acc 1.0\n",
      "Step 300 Loss 0.0044233073 Acc 1.0\n",
      "Step 400 Loss 0.37781703 Acc 0.8841871\n",
      "Step 500 Loss 0.0049841367 Acc 1.0\n",
      "Step 600 Loss 0.002878683 Acc 1.0\n",
      "Step 700 Loss 0.0022189005 Acc 1.0\n",
      "Step 800 Loss 0.0014595168 Acc 1.0\n",
      "Step 900 Loss 0.0015087313 Acc 1.0\n",
      "Accuracy with max 4 digits, evaluating on 5 digits: 0.0\n",
      "Step 100 Loss 0.022639556 Acc 0.99959135\n",
      "Step 200 Loss 0.0062896865 Acc 1.0\n",
      "Step 300 Loss 0.004093023 Acc 1.0\n",
      "Step 400 Loss 0.003018614 Acc 1.0\n",
      "Step 500 Loss 0.0017153687 Acc 1.0\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "ValueError in eval: substring not found\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "ValueError in eval: substring not found\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "SyntaxError in eval: invalid token (<string>, line 1)\n",
      "ValueError in eval: substring not found\n",
      "ValueError in eval: substring not found\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "ValueError in eval: substring not found\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "ValueError in eval: Unknown format code 'd' for object of type 'float'\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "ValueError in eval: substring not found\n",
      "SyntaxError in eval: unexpected EOF while parsing (<string>, line 1)\n",
      "Accuracy with max 6 digits, evaluating on 7 digits: 0.03\n",
      "Step 100 Loss 0.080279 Acc 0.98782235\n",
      "Step 200 Loss 0.030192997 Acc 1.0\n",
      "Step 300 Loss 0.017486505 Acc 1.0\n",
      "Step 400 Loss 0.027168302 Acc 0.9977843\n",
      "Step 500 Loss 0.013543472 Acc 0.9996245\n",
      "Step 600 Loss 0.00742637 Acc 1.0\n",
      "Step 700 Loss 0.0062103667 Acc 1.0\n",
      "Step 800 Loss 0.0049840794 Acc 0.9996345\n",
      "Step 900 Loss 0.004674073 Acc 1.0\n",
      "SyntaxError in eval: invalid syntax (<string>, line 1)\n",
      "SyntaxError in eval: invalid syntax (<string>, line 1)\n",
      "SyntaxError in eval: invalid syntax (<string>, line 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Q1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1e5eaabb344a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     print(\"Accuracy with max {} digits, evaluating on {} digits: {}\".format(max_n_digits, max_n_digits + 1, \n\u001b[1;32m     18\u001b[0m                                                                             accuracy))\n",
      "\u001b[0;32m<ipython-input-26-51f654b44fbc>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(train_dataset, test_dataset, model)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e5aca6c24acc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0910984d7983>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, a, b)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Call system 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtokenized_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-26652fe5c5f9>\u001b[0m in \u001b[0;36msystem_1\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msystem_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'{eval(s):02d}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msystem_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'7+1+0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Q1' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model(model_type=\"copy\")\n",
    "for max_n_digits in [2,3,4,6,8]:\n",
    "    train_dataset = data(\n",
    "        n_samples=10000,\n",
    "        max_n_digits=max_n_digits,\n",
    "        seed=0)\n",
    "    train_dataset = list(train_dataset)\n",
    "\n",
    "    test_dataset = data(\n",
    "        n_samples=100,\n",
    "        max_n_digits=max_n_digits+1,\n",
    "        min_n_digits=max_n_digits+1,\n",
    "        seed=0)\n",
    "    test_dataset = list(test_dataset)\n",
    "\n",
    "    accuracy, _ = run_experiment(train_dataset, test_dataset, model)\n",
    "    print(\"Accuracy with max {} digits, evaluating on {} digits: {}\".format(max_n_digits, max_n_digits + 1, \n",
    "                                                                            accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
